[{"name":"吳昭正","email":"ccwu@ntut.edu.tw","latestUpdate":"2015-02-25 14:21:32","objective":"本課程將先從隨機變數的內容開始並主要介紹消息理論的基礎與應用。本課程的將涵蓋下列內容：\n(1) 熵值，相對性熵值與互訊息\n(2) 漸近等分理論\n(3) 隨機程序的熵速率\n(4) 資料壓縮\n(5) 消息理論與賭博\n(6) 頻道容量\n(7) 差別熵值\n(8) 高斯通道","schedule":"Week 1(2/25). Introduction\nWeek 2(3/4). Review of Random Variables\nWeek 3(3/11). Entropy/Relative Entropy\nWeek 4(3/18). Mutual Information/Jensen’s and Fano's Inequality\nWeek 5(3/25). Data compression\nWeek 6(4/1). Huffman/Shannon coding\nWeek 7(4/8). Channel capacity (I)\nWeek 8(4/15). Channel capacity (II)\nWeek 9(4/22). Midterm\nWeek 10(4/29). Rate Distortion Theory (I)\nWeek 11(5/6). Rate Distortion Theory (II)\nWeek 12(5/13). Rate Distortion Theory (III)\nWeek 13(5/20). Asymptotic equipartition property\nWeek 14(5/27). Entropy Rates of a Stochastic Process\nWeek 15(6/3). Differential entropy\nWeek 16(6/10). Gaussian Channel\nWeek 17(6/17). Final Project Presentation\nWeek 18(6/24). Final term","scorePolicy":"Midterm 30%\nFinal term 30%\nTerm Project 40%","materials":"Required text: \n1. Elements of Information Theory, 2nd Edition, Thomas M. Cover and Joy A. Thomas, Wiley, 2006.\nOptional text: \n1. The Mathematical Theory of Information, Jan Kahre, The Springer, 2002.\n2. Information Theory, Inteference, and Learning Algorithms, David J. C. Mackay, Cambridge, 2003.\n3. Information Theory: Coding Theorems for Discrete Memoryless Systems, 2md Edition, Imre Csiszar and Janos Korner, Cambridge, 2011","foreignLanguageTextbooks":false}]
