[{"name":"尤信程","email":"scyou@ntut.edu.tw","latestUpdate":"2018-02-27 10:28:28","objective":"本課程以深入淺出之方式，介紹與機器學習之相關技術及演算法，目標是修習本課程之學生具有閱讀機器學習相關文獻之基本知識，並能從事相關之實務或電腦模擬實驗之工作，因此本課程著重在廣泛介紹各類之演算法之應用，而非深究少數演算法之深奧數學原理．本課程涵蓋之演算法，包含：最佳化基本知識，隨機最佳化演算法，隨機搜尋，決策樹，隨機森林，類神經網路及深度學習相關演算法，基因演算法，貝氏定理及應用，高斯混合模型，隱藏式馬可夫鍊及訓練演算法，支援向量機，線性及邏輯回歸，分群方法，降維方法，提升方法，決策融合技巧，及增強式學習等．","schedule":"Week 1: Class announcement and introduction to machine learning (CH 1)\nWeek 2: Basics of spervised learning (Example: K NN), VC dimension, and regression (CH 2). Baysian decision theory (CH 3) and Naive Bayes classifiers (CH 5) \nWeek 3: ML and MAP estimation; Bias vs variance dilemma (CH 4). Sample mean mean and sample covariance (CH 5). Dimension reduction techniques: PCA, FA (CH 6)\nWeek 4: Dimension reduction techniques: LDA and ICA. Clustering algorithm: k-mean (CH 7)\nWeek 5: Unsupervised neural networks (CH 12): Competitive learning, SOFM, and ART. Decision trees: ID3, C4.5, and random forest (CH 8)\nWeek 6: Holiday\nWeek 7: Decision trees (continued). Mixer model: GMM and EM algorithm (CH 7)\nWeek 8: Genetic algorithms. Basics of optiminization.\nWeek 9: MT\nWeek 10: MT sol. Gradient search and linear discrimination (CH 10)\nWeek 11: Feedforward neural networks with examples: radical basis networks and multi-layer perceptrons. Back propagation and regularization methods (CH 11)\nWeek 12: Deep learning and convolutional neural networks\nWeek 13: More on deep neural networks: Autoencoder, LSTM, and others \nWeek 14: SVM (CH 13) and HMM (CH 15)\nWeek 15: Combining multiple classifiers (CH 17)\nWeek 16: Design and analysis of experiments (CH 19)\nWeek 17: Reinforcement learning (CH 18)\nWeek 18: Final exam","scorePolicy":"MT 30 %\nFinal 40 %\nHW 30 %\nProject 10 % (optional)","materials":"Reference text book: Introduction to machine learning. E. Alpaydin. 2nd ed or 3rd ed. Note: textbook is only used to follow the presentation order. Much of the detailed lecture materials are NOT covered in the textbook.","foreignLanguageTextbooks":true}]
